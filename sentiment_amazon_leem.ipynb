{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis using Amazon reviews - Ernest Leem\n",
    "# https://erleem.medium.com/nlp-complete-sentiment-analysis-on-amazon-reviews-374e4fea9976\n",
    "# runs correctly -egs-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suchanek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/suchanek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/suchanek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/suchanek/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/suchanek/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, gzip \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "with gzip.open('reviews_Electronics_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/846n4xzj073b5sjsmq1f0z3c0000gp/T/ipykernel_34086/878245883.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "/var/folders/3x/846n4xzj073b5sjsmq1f0z3c0000gp/T/ipykernel_34086/878245883.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "/var/folders/3x/846n4xzj073b5sjsmq1f0z3c0000gp/T/ipykernel_34086/878245883.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "/var/folders/3x/846n4xzj073b5sjsmq1f0z3c0000gp/T/ipykernel_34086/878245883.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "/var/folders/3x/846n4xzj073b5sjsmq1f0z3c0000gp/T/ipykernel_34086/878245883.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n"
     ]
    }
   ],
   "source": [
    "# drop any rows w/ missing values\n",
    "df = df.dropna()\n",
    "# discover the actual counts\n",
    "df.overall.value_counts()\n",
    "# set sample size to labels w/ minimum count\n",
    "sample_size = 50000\n",
    "df_equal_overall = pd.DataFrame()\n",
    "for i in df.overall.unique():\n",
    "  X = df[df.overall == i].sample(sample_size)\n",
    "  df_equal_overall = df_equal_overall.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def ReviewProcessing(df):\n",
    "  # remove non alphanumeric \n",
    "  df['review_cleaned'] = df.reviewText.str.replace('[^a-zA-Z0-9 ]', '')\n",
    "  # lowercase\n",
    "  df.review_cleaned = df.review_cleaned.str.lower()\n",
    "  # split into list\n",
    "  df.review_cleaned = df.review_cleaned.str.split(' ')\n",
    "  # remove stopwords\n",
    "  df.review_cleaned = df.review_cleaned.apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def get_lemmatize(sent):\n",
    "  return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/846n4xzj073b5sjsmq1f0z3c0000gp/T/ipykernel_34086/2005400237.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review_cleaned'] = df.reviewText.str.replace('[^a-zA-Z0-9 ]', '')\n"
     ]
    }
   ],
   "source": [
    "clean_data = ReviewProcessing(df_equal_overall)\n",
    "clean_data.review_cleaned = clean_data.review_cleaned.apply(' '.join)\n",
    "clean_data['review_cleaned_lemmatized'] = clean_data.review_cleaned.apply(get_lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = Pipeline([('vectorize', CountVectorizer(ngram_range=(1, 2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier()),\n",
    "               ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(max_iter=500)),\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_data['review_cleaned_lemmatized']\n",
    "y = clean_data['overall']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                    test_size=0.2, stratify=y, random_state = 44)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46084\n",
      "[[5762 3001  766  361  110]\n",
      " [2459 4449 2003  962  127]\n",
      " [1092 2555 3393 2695  265]\n",
      " [ 532 1066 1593 5731 1078]\n",
      " [ 579  664  689 4361 3707]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.55      0.58      0.56     10000\n",
      "         2.0       0.38      0.44      0.41     10000\n",
      "         3.0       0.40      0.34      0.37     10000\n",
      "         4.0       0.41      0.57      0.48     10000\n",
      "         5.0       0.70      0.37      0.48     10000\n",
      "\n",
      "    accuracy                           0.46     50000\n",
      "   macro avg       0.49      0.46      0.46     50000\n",
      "weighted avg       0.49      0.46      0.46     50000\n",
      "\n",
      "0.4744\n",
      "[[8063  665  410  260  602]\n",
      " [5009 1905 1333  757  996]\n",
      " [2124 1396 2684 2000 1796]\n",
      " [ 788  461 1025 3173 4553]\n",
      " [ 624  165  333  983 7895]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.49      0.81      0.61     10000\n",
      "         2.0       0.41      0.19      0.26     10000\n",
      "         3.0       0.46      0.27      0.34     10000\n",
      "         4.0       0.44      0.32      0.37     10000\n",
      "         5.0       0.50      0.79      0.61     10000\n",
      "\n",
      "    accuracy                           0.47     50000\n",
      "   macro avg       0.46      0.47      0.44     50000\n",
      "weighted avg       0.46      0.47      0.44     50000\n",
      "\n",
      "0.51044\n",
      "[[6512 2140  685  257  406]\n",
      " [2848 3947 1982  673  550]\n",
      " [1060 2049 3830 2085  976]\n",
      " [ 406  618 1738 4518 2720]\n",
      " [ 369  278  570 2068 6715]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.58      0.65      0.61     10000\n",
      "         2.0       0.44      0.39      0.41     10000\n",
      "         3.0       0.43      0.38      0.41     10000\n",
      "         4.0       0.47      0.45      0.46     10000\n",
      "         5.0       0.59      0.67      0.63     10000\n",
      "\n",
      "    accuracy                           0.51     50000\n",
      "   macro avg       0.50      0.51      0.51     50000\n",
      "weighted avg       0.50      0.51      0.51     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_nb))\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# SGD Classifier\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_sgd))\n",
    "print(confusion_matrix(y_test, y_pred_sgd))\n",
    "print(classification_report(y_test, y_pred_sgd))\n",
    "\n",
    "# Logistic Regression\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_log = logreg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_log))\n",
    "print(confusion_matrix(y_test, y_pred_log))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid=[{'clf__solver': ['lbfgs', 'sag', 'saga'],\n",
    "       'clf__C': [0.01, 0.1, 1]}]\n",
    "lr = GridSearchCV(logreg, param_grid = grid, cv = 5, scoring='accuracy', verbose = 1, n_jobs = -1)\n",
    "best_model = lr.fit(X_train, y_train)\n",
    "\n",
    "print(best_model.best_estimator_)\n",
    "print(best_model.best_score_)\n",
    "\n",
    "y_pred_grid = best_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_grid))\n",
    "print(classification_report(y_test, y_pred_grid))\n",
    "print(accuracy_score(y_test, y_pred_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (ai_m1)",
   "language": "python",
   "name": "ai_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
